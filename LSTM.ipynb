{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "L4fM4nA67kqM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import tensorflow as tf\n",
        "from keras.layers import Input, Embedding, Activation, Flatten, Dense\n",
        "from keras.layers import Conv1D, MaxPooling1D, Dropout\n",
        "from keras.models import Model\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Embedding\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NIHx2XXcA96q",
        "outputId": "6032d3b0-4072-4e23-8e76-1b3d590b654e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data=pd.read_csv('/content/gdrive/My Drive/dataset_7_features.csv')\n",
        "data=np.array(data)\n",
        "X=data[:,0]\n",
        "y_label=data[:,1]\n",
        "print(len(X))\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, y_label, test_size=0.2)\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(X_train,Y_train, test_size=0.1)"
      ],
      "metadata": {
        "id": "B0EY-rud8o_Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d89754f-7d6c-4c09-8eb0-d59b1e7c70c4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1018002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_texts=[str(s) for s in X_train]\n",
        "test_texts=[str(s) for s in X_test]\n",
        "val_texts=[str(s) for s in X_val]"
      ],
      "metadata": {
        "id": "-Gn4DW1UBKcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizer\n",
        "tk = Tokenizer(num_words=None, char_level=True, oov_token='UNK')\n",
        "tk.fit_on_texts(train_texts)\n",
        "\n",
        "# construct a new vocabulary\n",
        "alphabet = \"abcdefghijklmnopqrstuvwxyz0123456789\"\n",
        "char_dict = {}\n",
        "for i, char in enumerate(alphabet):\n",
        "    char_dict[char] = i + 1\n",
        "\n",
        "# Use char_dict to replace the tk.word_index\n",
        "tk.word_index = char_dict.copy()\n",
        "# Add 'UNK' to the vocabulary\n",
        "tk.word_index[tk.oov_token] = max(char_dict.values()) + 1\n",
        "\n",
        "# Convert string to index\n",
        "train_sequences = tk.texts_to_sequences(train_texts)\n",
        "test_texts = tk.texts_to_sequences(test_texts)\n",
        "val_texts = tk.texts_to_sequences(val_texts)\n",
        "\n",
        "# Padding\n",
        "train_data = pad_sequences(train_sequences,maxlen=75, padding='post')\n",
        "test_data = pad_sequences(test_texts,maxlen=75,padding='post')\n",
        "val_data = pad_sequences(val_texts,maxlen=75,padding='post')\n",
        "\n",
        "\n",
        "# Convert to numpy array\n",
        "train_data = np.array(train_data, dtype='float32')\n",
        "test_data = np.array(test_data, dtype='float32')\n",
        "val_data = np.array(val_data, dtype='float32')\n"
      ],
      "metadata": {
        "id": "F8ymx5rm9jmD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tk.word_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4AuuoGy4KMXG",
        "outputId": "4f87071f-ae81-4f8d-eaf8-8757c5598c49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'0': 27,\n",
              " '1': 28,\n",
              " '2': 29,\n",
              " '3': 30,\n",
              " '4': 31,\n",
              " '5': 32,\n",
              " '6': 33,\n",
              " '7': 34,\n",
              " '8': 35,\n",
              " '9': 36,\n",
              " 'UNK': 37,\n",
              " 'a': 1,\n",
              " 'b': 2,\n",
              " 'c': 3,\n",
              " 'd': 4,\n",
              " 'e': 5,\n",
              " 'f': 6,\n",
              " 'g': 7,\n",
              " 'h': 8,\n",
              " 'i': 9,\n",
              " 'j': 10,\n",
              " 'k': 11,\n",
              " 'l': 12,\n",
              " 'm': 13,\n",
              " 'n': 14,\n",
              " 'o': 15,\n",
              " 'p': 16,\n",
              " 'q': 17,\n",
              " 'r': 18,\n",
              " 's': 19,\n",
              " 't': 20,\n",
              " 'u': 21,\n",
              " 'v': 22,\n",
              " 'w': 23,\n",
              " 'x': 24,\n",
              " 'y': 25,\n",
              " 'z': 26}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size=len(tk.word_index)\n",
        "vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JpArNchME4NV",
        "outputId": "a85493b9-eb23-4065-d808-024ac8c12de1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "37"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_domains=len(train_data)\n",
        "seq_length=len(train_data[0])\n",
        "train_X=np.reshape(train_data,(len(train_data),seq_length,1))\n",
        "test_X=np.reshape(test_data,(len(test_data),seq_length,1))\n",
        "val_X=np.reshape(val_data,(len(val_data),seq_length,1))\n",
        "\n",
        "train_X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qW7fPnUWHT5b",
        "outputId": "efefbf55-2fe3-4aa3-c3a9-99f96c80e3b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(732960, 75, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embed=tf.keras.layers.Embedding(vocab_size+1,1, input_length=75)\n",
        "train_eX=embed(train_data)\n",
        "test_eX=embed(test_data)\n",
        "val_eX=embed(val_data)"
      ],
      "metadata": {
        "id": "dnNHqNdCJhaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_eX.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYlh758_MQtG",
        "outputId": "29a1896a-06f2-4880-9570-d8272e655bc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([732960, 75, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# one hot encodes the output variable\n",
        "train_y = np_utils.to_categorical(Y_train)\n",
        "test_y=np_utils.to_categorical(Y_test)\n",
        "val_y=np_utils.to_categorical(Y_val)\n"
      ],
      "metadata": {
        "id": "eMBCigfbIZAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model 1\n",
        "embedding_dim =64\n",
        "max_length =75\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim, input_length=max_length))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(1))\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy', 'AUC'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dh3xH1aNP-BM",
        "outputId": "59c2f54a-35b4-43e9-c544-cf67a5881643"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 75, 64)            2368      \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 100)               66000     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 100)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 101       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 68,469\n",
            "Trainable params: 68,469\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#model 2\n",
        "model2 = tf.keras.Sequential([\n",
        "   # tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(75)),\n",
        "   # tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(2,activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "model2.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "              metrics=['accuracy', 'AUC'])\n",
        "\n",
        "model2.build(input_shape=(None,75,1))\n",
        "model2.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eViq7WlgHoaI",
        "outputId": "c98de52d-4f4e-4b01-dcf8-3c30aef669ba"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bidirectional (Bidirectiona  (None, 150)              46200     \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                9664      \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 64)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2)                 130       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 55,994\n",
            "Trainable params: 55,994\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history=model2.fit(train_eX, train_y,validation_data=(val_eX,val_y) ,epochs = 10, batch_size=128)"
      ],
      "metadata": {
        "id": "5DxSXcaTVdI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('/content/gdrive/My Drive/models/lstm2_model2_50.h5')"
      ],
      "metadata": {
        "id": "7OclVmYvMobO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_graphs(history, metric):\n",
        "  plt.plot(history.history[metric])\n",
        "  plt.plot(history.history['val_'+metric], '')\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(metric)\n",
        "  plt.legend([metric, 'val_'+metric])\n",
        "\n",
        "\n",
        "plt.figure(figsize=(16, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plot_graphs(history, 'accuracy')\n",
        "plt.ylim(None, 1)\n",
        "plt.subplot(1, 2, 2)\n",
        "plot_graphs(history, 'loss')\n",
        "plt.ylim(0, None)\n"
      ],
      "metadata": {
        "id": "zS9usvUhVwHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_X=np.reshape(test_data,(len(test_data),63,1))\n",
        "test_X.shape\n",
        "predictions=model.predict(test_X)"
      ],
      "metadata": {
        "id": "9O4b5nnMqheO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_X = np.asarray(test_X).astype('float32')\n",
        "test_y=np.asarray(test_y).astype('float32')\n",
        "test_loss, test_acc = model.evaluate(test_X,test_y)\n",
        "\n",
        "print('Test Loss:', test_loss)\n",
        "print('Test Accuracy:', test_acc)\n"
      ],
      "metadata": {
        "id": "4dywXlI6tHXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.load_model('/content/gdrive/My Drive/models/lstm2_n.h5')"
      ],
      "metadata": {
        "id": "3JxuHEPqvA0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FGSM attack:"
      ],
      "metadata": {
        "id": "Js-sR-q4cJai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fast_gradient_method(\n",
        "    model_fn,\n",
        "    x,\n",
        "    eps,\n",
        "    norm,\n",
        "    loss_fn=None,\n",
        "    clip_min=None,\n",
        "    clip_max=None,\n",
        "    y=None,\n",
        "    targeted=False,\n",
        "    iterative=False,\n",
        "    sanity_checks=False,\n",
        "):\n",
        "\n",
        "    if y is None:\n",
        "        y=np.argmax(model(x))\n",
        "\n",
        "    if loss_fn is None:\n",
        "      loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "\n",
        "    if iterative:#if iterative, give also iters\n",
        "      adv_X,optimal_perturbation=iterative_fgsm(model_fn,x,y,targeted,iters)\n",
        "\n",
        "    \n",
        "    #cast if numpy array is given\n",
        "    x = tf.cast(x, tf.float32)\n",
        "\n",
        "    #function to compute gradients\n",
        "    grad = compute_gradient(model_fn, loss_fn, x, y, targeted,iters=1)\n",
        "    \n",
        "    #function to find optimal perturbation based on norm\n",
        "    optimal_perturbation=norm_constraint(grad,eps,norm)\n",
        "    #adv image created\n",
        "    adv_x=x+optimal_perturbation\n",
        "\n",
        "    norm=l2(adv_x,x)\n",
        "      \n",
        "    preds=model.predict(adv_x)\n",
        "    lables=np.argmax(preds,axis=1)\n",
        "    print(preds,lables)\n",
        "      \n",
        "    confidence=np.max(preds,axis=1)*100\n",
        "    print(confidence)\n",
        "\n",
        "\n",
        "    return adv_x\n"
      ],
      "metadata": {
        "id": "YoAWmnFLcHe3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_gradient(model,loss_fn,x,y,targeted,iters):\n",
        "    for epoch in range(iters):\n",
        "      with tf.GradientTape() as gt:\n",
        "        gt.watch(x)\n",
        "        label=model(x)\n",
        "        loss=loss_fn(y,label)\n",
        "        #print(loss)\n",
        "        if(targeted):\n",
        "          loss=-loss\n",
        "\n",
        "    grad=gt.gradient(loss,x)\n",
        "    #print(grad)\n",
        "    return grad"
      ],
      "metadata": {
        "id": "27xf_7hVYBk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def norm_constraint(grad, eps, norm=np.inf):\n",
        "    \"\"\"\n",
        "    Solves for the optimal input to a linear function under a norm constraint.\n",
        "    Optimal_perturbation = argmax_{eta, ||eta||_{norm} < eps} dot(eta, grad)\n",
        "    :param grad: tf tensor containing a batch of gradients\n",
        "    :param eps: float scalar specifying size of constraint region\n",
        "    :param norm: int specifying order of norm\n",
        "    :returns:\n",
        "      tf tensor containing optimal perturbation\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert the iterator returned by `range` into a list.\n",
        "    axis = list(range(1, len(grad.get_shape())))\n",
        "    avoid_zero_div = 1e-12\n",
        "    if norm == np.inf:\n",
        "        # Take sign of gradient\n",
        "        optimal_perturbation = tf.sign(grad)\n",
        "        # The following line should not change the numerical results. It applies only because\n",
        "        # `optimal_perturbation` is the output of a `sign` op, which has zero derivative anyway.\n",
        "        # It should not be applied for the other norms, where the perturbation has a non-zero derivative.\n",
        "        optimal_perturbation = tf.stop_gradient(optimal_perturbation)\n",
        "    elif norm == 1:\n",
        "        abs_grad = tf.abs(grad)\n",
        "        sign = tf.sign(grad)\n",
        "        max_abs_grad = tf.reduce_max(abs_grad, axis, keepdims=True)#maximum value of gradient\n",
        "        tied_for_max = tf.dtypes.cast(\n",
        "            tf.equal(abs_grad, max_abs_grad), dtype=tf.float32\n",
        "        )\n",
        "        num_ties = tf.reduce_sum(tied_for_max, axis, keepdims=True)\n",
        "        optimal_perturbation = sign * tied_for_max / num_ties\n",
        "    elif norm == 2:\n",
        "        square = tf.maximum(\n",
        "            avoid_zero_div, tf.reduce_sum(tf.square(grad), axis, keepdims=True)\n",
        "        )\n",
        "        optimal_perturbation = grad / tf.sqrt(square)\n",
        "    else:\n",
        "        raise NotImplementedError(\n",
        "            \"Only L-inf, L1 and L2 norms are currently implemented.\"\n",
        "        )\n",
        "\n",
        "    # Scale perturbation to be the solution for the norm=eps rather than norm=1 problem\n",
        "    scaled_perturbation = tf.multiply(eps, optimal_perturbation)\n",
        "    return scaled_perturbation"
      ],
      "metadata": {
        "id": "_tS7XqEQYFVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def l2(x, y):\n",
        "  # technically squarred l2\n",
        "    return tf.reduce_sum(tf.square(x - y), list(range(1, len(x.shape))))"
      ],
      "metadata": {
        "id": "bx2e9EHgfaqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rev_ind= dict((tk.word_index[k], k) for k in tk.word_index)\n",
        "def convert_to_domain(adv_x,vocab_size,word_ind):\n",
        "  domain=[]\n",
        "  inds=[]\n",
        "  adv_x=np.asarray(adv_x).reshape(1,75).flatten()\n",
        "\n",
        "  scale=lambda ind : ind*vocab_size if ind < 1 else ind\n",
        "  abs_inds=abs(adv_x)\n",
        "  inds=[round(scale(ind)) for ind in abs_inds]\n",
        "  domains=[rev_ind[ind] if ind!=0 else 0 for ind in inds]\n",
        "  dom=\"\"\n",
        "  for c in domains:\n",
        "    if c!=0:\n",
        "      dom+=c\n",
        "  print(dom)\n",
        "  return dom\n",
        "\n"
      ],
      "metadata": {
        "id": "tr72MRkzyA9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x= tk.texts_to_sequences(['sogupteabok'])\n",
        "#x= tk.texts_to_sequences(['google'])\n",
        "x=pad_sequences(x,maxlen=75, padding='post')\n",
        "x=np.array(x)\n",
        "x=x.reshape(1,75)\n",
        "embed=tf.keras.layers.Embedding(vocab_size+1,1, input_length=75)\n",
        "x=embed(x)\n",
        "print(x.shape)\n",
        "y=np.array([1,0]).reshape(1,2)\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dro_9XQdaCl8",
        "outputId": "fe9b498d-8a5a-4a0c-f17d-00d43fe8766f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 75, 1)\n",
            "[[1 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label=model(x)\n",
        "np.argmax(label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxvlTk8KdP5u",
        "outputId": "60961a04-f7d7-4b47-a377-55ae9cc140b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn=tf.keras.losses.CategoricalCrossentropy()\n",
        "eps=0.7\n",
        "t_lable=np.array([0,1]).reshape(1,2)\n",
        "adv_x=fast_gradient_method(model,x,eps,2,clip_min=-1,clip_max=1,y=t_lable,targeted=True,loss_fn=loss_fn)\n",
        "\n",
        "adv_domain=convert_to_domain(adv_x,vocab_size,rev_ind)"
      ],
      "metadata": {
        "id": "ejFCeYjJYMJR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20e2c8e6-742f-4c02-8f99-9bbe852f2eb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.42085698 0.579143  ]] [1]\n",
            "[57.9143]\n",
            "cfbbjcbaaiccaaaaaaaaaabbbbbbbbbbbbbbccccccccccccccccccdddddeeeeeeedddddcb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#FGSM training\n",
        "cnt=0\n",
        "adv_train_eX=[]\n",
        "for i in range(len(train_eX)):\n",
        "\n",
        "  x=train_eX[i]\n",
        "  x=np.asarray(x)\n",
        "  x=x.reshape(1,75,1)\n",
        "  t_lable=np.abs(1-train_y[i]).reshape(1,2)\n",
        "  eps=0.7\n",
        "  loss_fn=tf.keras.losses.CategoricalCrossentropy()\n",
        "  print(x.shape)\n",
        "  adv_x=fast_gradient_method(model,x,eps,2,clip_min=-1,clip_max=1,y=t_lable,targeted=True,loss_fn=loss_fn)\n",
        "  \n",
        "  preds=model.predict(adv_x)\n",
        "  lables=np.argmax(preds,axis=1)\n",
        "  print(Y_train[i],lables[0])\n",
        "  if Y_train[i]!=lables[0]:\n",
        "    adv_train_eX.append(adv_x)\n",
        "    cnt=cnt+1\n",
        "  else:\n",
        "    print('attack fail')\n",
        "\n",
        "adv_train_eX=np.array(adv_train_eX)\n",
        "print(adv_train_eX.shape)\n",
        "print(cnt)\n"
      ],
      "metadata": {
        "id": "bTjJ-2N7edYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8A8MPRlBlURd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}